{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lErNlWAsMc2P"
      },
      "source": [
        "# Laboratorio de Text Mining: Análisis de los Cuentos de Hernán Casciari\n",
        "\n",
        "**Palabras clave:** Text Mining, Corpus, Bag of Words, Stop Words, Vectorización, Análisis de Frecuencias\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Introducción al Text Mining\n",
        "\n",
        "### ¿Qué es Text Mining?\n",
        "\n",
        "El **Text Mining** (minería de texto) es el proceso de extraer información valiosa y patrones significativos de grandes volúmenes de texto no estructurado. A diferencia del análisis de datos tradicional que trabaja con números, el text mining nos permite:\n",
        "\n",
        "- Descubrir temas recurrentes en una colección de documentos\n",
        "- Analizar sentimientos y opiniones\n",
        "- Identificar patrones temporales en el uso del lenguaje\n",
        "- Extraer conocimiento de textos que sería imposible procesar manualmente\n",
        "\n",
        "### El Corpus: Los Cuentos de Hernán Casciari\n",
        "\n",
        "En este laboratorio trabajaremos con los cuentos del escritor argentino **Hernán Casciari**, publicados en su blog entre 2004 y 2015. Este corpus es especialmente interesante porque:\n",
        "\n",
        "1. **Dimensión temporal**: 12 años de escritura continua\n",
        "2. **Estilo personal**: Narrativa en primera persona, muy autobiográfica\n",
        "3. **Evolución temática**: Refleja cambios en la vida del autor\n",
        "4. **Lenguaje auténtico**: Español rioplatense contemporáneo\n",
        "\n",
        "### Objetivos del Análisis\n",
        "\n",
        "Nuestro objetivo principal es **explorar qué tenía Hernán Casciari en su mente** cuando escribía estos cuentos. Específicamente queremos:\n",
        "\n",
        "- Identificar las palabras y temas más recurrentes por año\n",
        "- Analizar la evolución de su vocabulario a lo largo del tiempo\n",
        "- Descubrir patrones que reflejen cambios en su vida personal\n",
        "- Visualizar estos hallazgos de manera comprensible\n",
        "\n",
        "### Metodología\n",
        "\n",
        "Seguiremos el pipeline típico de text mining:\n",
        "\n",
        "1. **Carga de datos**: Importar los textos desde archivos\n",
        "2. **Preprocesamiento**: Limpiar y normalizar el texto\n",
        "3. **Tokenización**: Dividir el texto en palabras individuales\n",
        "4. **Filtrado**: Eliminar palabras poco informativas (stop words)\n",
        "5. **Vectorización**: Convertir texto a representación numérica\n",
        "6. **Análisis**: Calcular frecuencias y estadísticas\n",
        "7. **Visualización**: Crear gráficos y nubes de palabras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXe4RILWMc2T"
      },
      "source": [
        "---\n",
        "\n",
        "## 2. Preparación del Entorno\n",
        "\n",
        "### Librerías Necesarias\n",
        "\n",
        "Antes de comenzar, importemos todas las librerías que utilizaremos y entendamos para qué sirve cada una:\n",
        "\n",
        "- **pandas**: Manipulación y análisis de datos estructurados\n",
        "- **numpy**: Operaciones numéricas eficientes\n",
        "- **re**: Procesamiento con expresiones regulares\n",
        "- **string**: Herramientas para trabajar con cadenas de texto\n",
        "- **pickle**: Serialización de objetos Python\n",
        "- **nltk**: Natural Language Toolkit, biblioteca fundamental de NLP\n",
        "- **sklearn**: Herramientas de machine learning, incluyendo vectorización\n",
        "- **wordcloud**: Generación de nubes de palabras\n",
        "- **matplotlib**: Creación de visualizaciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsbJ7IUzMc2T"
      },
      "outputs": [],
      "source": [
        "# Importación de librerías necesarias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "# Librerías de NLP\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Librerías de visualización\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Configuración para Google Colab\n",
        "from google.colab import drive\n",
        "\n",
        "# Configuración de pandas para mostrar más contenido en las celdas\n",
        "pd.set_option('max_colwidth', 150)\n",
        "\n",
        "print(\"Todas las librerías importadas correctamente\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7GsJYlBMc2U"
      },
      "source": [
        "### Conexión con Google Drive\n",
        "\n",
        "Para acceder a nuestros datos, necesitamos montar Google Drive:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhjulKp0Mc2U"
      },
      "outputs": [],
      "source": [
        "# Montar Google Drive para acceder a los archivos\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Definir la ruta donde están almacenados los cuentos\n",
        "path = '/content/drive/MyDrive/Clases/HABLA/003/PRA/cuentos_casciari/'\n",
        "\n",
        "print(\"Google Drive montado exitosamente\")\n",
        "print(f\"Ruta de trabajo: {path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdK8ZRsnMc2U"
      },
      "source": [
        "---\n",
        "\n",
        "## 3. Carga y Exploración de Datos\n",
        "\n",
        "### Estructura del Corpus\n",
        "\n",
        "Nuestro corpus consiste en **12 archivos** correspondientes a los cuentos publicados por Casciari entre 2004 y 2015. Cada archivo contiene todos los textos de un año específico.\n",
        "\n",
        "**Formato de archivos:**\n",
        "- Archivos `.txt` que han sido *pickleados* (serializados con pickle)\n",
        "- Un archivo por año: `2004.txt`, `2005.txt`, ..., `2015.txt`\n",
        "- Cada archivo contiene un único string con todos los cuentos del año"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkubOpBsMc2U"
      },
      "outputs": [],
      "source": [
        "# Definir los años disponibles en nuestro corpus\n",
        "anios = ['2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015']\n",
        "\n",
        "# Diccionario para almacenar los textos de cada año\n",
        "data = {}\n",
        "\n",
        "# Contador para verificar la carga\n",
        "archivos_cargados = 0\n",
        "archivos_fallidos = 0\n",
        "\n",
        "print(\"Iniciando carga de archivos...\")\n",
        "print(\"-\" * 40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuI0KqzqMc2V"
      },
      "outputs": [],
      "source": [
        "# Cargar cada archivo de manera segura con manejo de errores\n",
        "for anio in anios:\n",
        "    nombre_archivo = path + anio + '.txt'\n",
        "    try:\n",
        "        with open(nombre_archivo, \"rb\") as archivo:\n",
        "            data[anio] = pickle.load(archivo)\n",
        "\n",
        "        # Mostrar información básica del archivo cargado\n",
        "        num_caracteres = len(data[anio])\n",
        "        num_palabras_aprox = len(data[anio].split())\n",
        "\n",
        "        print(f\"✓ {anio}: {num_caracteres:,} caracteres, ~{num_palabras_aprox:,} palabras\")\n",
        "        archivos_cargados += 1\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"✗ No se encontró el archivo {anio}.txt\")\n",
        "        archivos_fallidos += 1\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error cargando {anio}.txt: {e}\")\n",
        "        archivos_fallidos += 1\n",
        "\n",
        "print(\"-\" * 40)\n",
        "print(f\"Resumen: {archivos_cargados} archivos cargados, {archivos_fallidos} fallos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IILrVVywMc2V"
      },
      "source": [
        "### Exploración Inicial del Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlb7PLhfMc2V"
      },
      "outputs": [],
      "source": [
        "# Verificar que los datos se cargaron correctamente\n",
        "print(\"Años disponibles en el corpus:\")\n",
        "print(list(data.keys()))\n",
        "print()\n",
        "\n",
        "# Mostrar un fragmento del primer año para entender el formato\n",
        "print(\"Fragmento del año 2004 (primeros 500 caracteres):\")\n",
        "print(\"-\" * 50)\n",
        "print(data['2004'][0:500])\n",
        "print(\"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWo_8IOIMc2W"
      },
      "source": [
        "### Conversión a DataFrame\n",
        "\n",
        "Para facilitar el análisis, convertiremos nuestro diccionario en un **DataFrame de pandas**. Esto nos permitirá manipular los datos de manera más eficiente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UuE93_oMc2W"
      },
      "outputs": [],
      "source": [
        "# Convertir el diccionario a formato adecuado para DataFrame\n",
        "# Necesitamos que cada valor sea una lista para que pandas pueda crear el DataFrame\n",
        "data_combined = {key: [value] for (key, value) in data.items()}\n",
        "\n",
        "# Crear DataFrame\n",
        "data_df = pd.DataFrame.from_dict(data_combined).transpose()\n",
        "data_df.columns = ['transcript']  # Renombrar la columna\n",
        "data_df = data_df.sort_index()    # Ordenar por año\n",
        "\n",
        "print(\"DataFrame creado exitosamente\")\n",
        "print(f\"Dimensiones: {data_df.shape}\")\n",
        "print()\n",
        "print(\"Estructura del DataFrame:\")\n",
        "print(data_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7eiE0QKMc2W"
      },
      "source": [
        "### Pregunta de Reflexión\n",
        "\n",
        "**Antes de continuar, reflexiona:**\n",
        "1. ¿Qué observas en el fragmento de texto mostrado?\n",
        "2. ¿Qué elementos del texto podrían interferir con nuestro análisis?\n",
        "3. ¿Por qué crees que necesitamos \"limpiar\" el texto antes de analizarlo?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnZ4pM9DMc2W"
      },
      "source": [
        "## 4. Preprocesamiento de Texto\n",
        "\n",
        "### Teoría: ¿Por qué Limpiar el Texto?\n",
        "\n",
        "Los textos \"en crudo\" contienen muchos elementos que pueden interferir con nuestro análisis:\n",
        "\n",
        "1. **Signos de puntuación**: No aportan información semántica relevante\n",
        "2. **Mayúsculas y minúsculas**: \"Casa\" y \"casa\" deben tratarse como la misma palabra\n",
        "3. **Números**: Pueden distorsionar el análisis de patrones lingüísticos\n",
        "4. **Espacios extras y caracteres especiales**: Ruido en los datos\n",
        "\n",
        "### Expresiones Regulares: Una Herramienta Poderosa\n",
        "\n",
        "Las **expresiones regulares** nos permiten encontrar y reemplazar patrones en texto. Algunos patrones básicos:\n",
        "\n",
        "- `.`: Cualquier carácter\n",
        "- `*`: Cero o más repeticiones\n",
        "- `+`: Una o más repeticiones\n",
        "- `[abc]`: Cualquier carácter de los especificados\n",
        "- `\\d`: Cualquier dígito\n",
        "- `\\w`: Cualquier carácter de palabra\n",
        "\n",
        "### Primera Ronda de Limpieza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4O3dtOEMc2W"
      },
      "outputs": [],
      "source": [
        "# Explorar los signos de puntuación que vamos a eliminar\n",
        "print(\"Signos de puntuación estándar:\")\n",
        "print(string.punctuation)\n",
        "print()\n",
        "print(\"Versión escapada para expresiones regulares:\")\n",
        "print(re.escape(string.punctuation))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URx1-pZ_Mc2W"
      },
      "outputs": [],
      "source": [
        "def clean_text_round1(text):\n",
        "    \"\"\"\n",
        "    Primera ronda de limpieza de texto.\n",
        "\n",
        "    Parámetros:\n",
        "    text (str): Texto a limpiar\n",
        "\n",
        "    Retorna:\n",
        "    str: Texto limpio\n",
        "\n",
        "    Procesos aplicados:\n",
        "    1. Conversión a minúsculas\n",
        "    2. Remoción de texto entre corchetes\n",
        "    3. Remoción de signos de puntuación\n",
        "    4. Remoción de palabras que contienen números\n",
        "    5. Limpieza de espacios extra\n",
        "    \"\"\"\n",
        "    # Paso 1: Convertir a minúsculas para normalizar\n",
        "    text = text.lower()\n",
        "\n",
        "    # Paso 2: Remover texto entre corchetes (referencias, notas, etc.)\n",
        "    text = re.sub(r'\\[.*?\\]', ' ', text)\n",
        "\n",
        "    # Paso 3: Remover signos de puntuación y reemplazarlos con espacios\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
        "\n",
        "    # Paso 4: Remover palabras que contienen números\n",
        "    text = re.sub(r'\\w*\\d\\w*', ' ', text)\n",
        "\n",
        "    # Paso 5: Limpiar espacios múltiples y espacios al inicio/final\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0W-9eu8jMc2W"
      },
      "outputs": [],
      "source": [
        "# Ejemplo de cómo funciona la limpieza\n",
        "texto_ejemplo = \"¡Hola! ¿Cómo estás? Tengo 25 años y vivo en Buenos Aires.\"\n",
        "texto_limpio = clean_text_round1(texto_ejemplo)\n",
        "\n",
        "print(\"Texto original:\")\n",
        "print(texto_ejemplo)\n",
        "print()\n",
        "print(\"Texto después de limpieza:\")\n",
        "print(texto_limpio)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funcion de limpieza mejorada"
      ],
      "metadata": {
        "id": "lGy_dHRzQhla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text_round1(text):\n",
        "    \"\"\"\n",
        "    Primera ronda de limpieza de texto.\n",
        "\n",
        "    Parámetros:\n",
        "    text (str): Texto a limpiar\n",
        "\n",
        "    Retorna:\n",
        "    str: Texto limpio\n",
        "\n",
        "    Procesos aplicados:\n",
        "    1. Conversión a minúsculas\n",
        "    2. Remoción de texto entre corchetes\n",
        "    3. Remoción de signos de puntuación (incluyendo acentos españoles)\n",
        "    4. Remoción de palabras que contienen números\n",
        "    5. Limpieza de espacios extra\n",
        "    \"\"\"\n",
        "    # Paso 1: Convertir a minúsculas para normalizar\n",
        "    text = text.lower()\n",
        "\n",
        "    # Paso 2: Remover texto entre corchetes (referencias, notas, etc.)\n",
        "    text = re.sub(r'\\[.*?\\]', ' ', text)\n",
        "\n",
        "    # Paso 3: Remover signos de puntuación (incluyendo signos españoles)\n",
        "    # string.punctuation no incluye ¡¿, así que los agregamos manualmente\n",
        "    punctuation_extended = string.punctuation + '¡¿\"\"''–—'\n",
        "    text = re.sub('[%s]' % re.escape(punctuation_extended), ' ', text)\n",
        "\n",
        "    # Paso 4: Remover palabras que contienen números\n",
        "    text = re.sub(r'\\w*\\d\\w*', ' ', text)\n",
        "\n",
        "    # Paso 5: Limpiar espacios múltiples y espacios al inicio/final\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "7PIZnlY0Qd4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de cómo funciona la limpieza\n",
        "texto_ejemplo = \"¡Hola! ¿Cómo estás? Tengo 25 años y vivo en Buenos Aires.\"\n",
        "texto_limpio = clean_text_round1(texto_ejemplo)\n",
        "\n",
        "print(\"Texto original:\")\n",
        "print(texto_ejemplo)\n",
        "print()\n",
        "print(\"Texto después de limpieza:\")\n",
        "print(texto_limpio)"
      ],
      "metadata": {
        "id": "u_0M27b5QoYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b6WMgKLMc2W"
      },
      "outputs": [],
      "source": [
        "# Aplicar la primera limpieza a todo nuestro corpus\n",
        "print(\"Aplicando primera ronda de limpieza...\")\n",
        "\n",
        "def aplicar_limpieza_corpus(dataframe):\n",
        "    \"\"\"\n",
        "    Aplica la función de limpieza a todo el corpus.\n",
        "\n",
        "    Esta función es más explícita que usar lambda,\n",
        "    lo que facilita la comprensión para principiantes.\n",
        "    \"\"\"\n",
        "    textos_limpios = []\n",
        "\n",
        "    for i, texto in enumerate(dataframe.transcript):\n",
        "        texto_limpio = clean_text_round1(texto)\n",
        "        textos_limpios.append(texto_limpio)\n",
        "\n",
        "        # Mostrar progreso cada 4 archivos\n",
        "        if (i + 1) % 4 == 0:\n",
        "            print(f\"Procesados {i + 1} de {len(dataframe)} archivos\")\n",
        "\n",
        "    return pd.DataFrame(textos_limpios, columns=['transcript'], index=dataframe.index)\n",
        "\n",
        "# Aplicar la limpieza\n",
        "data_clean_round1 = aplicar_limpieza_corpus(data_df)\n",
        "print(\"Primera limpieza completada\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKPfPy72Mc2W"
      },
      "source": [
        "### Segunda Ronda de Limpieza\n",
        "\n",
        "Algunos caracteres especiales requieren atención adicional:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQJ-SxvdMc2W"
      },
      "outputs": [],
      "source": [
        "def clean_text_round2(text):\n",
        "    \"\"\"\n",
        "    Segunda ronda de limpieza: caracteres especiales y formato.\n",
        "\n",
        "    Procesos aplicados:\n",
        "    1. Remoción de comillas especiales y puntos suspensivos\n",
        "    2. Normalización de saltos de línea\n",
        "    \"\"\"\n",
        "    # Remover comillas especiales, puntos suspensivos, y comillas de diálogo\n",
        "    text = re.sub('[''\"\"…«»]', '', text)\n",
        "\n",
        "    # Reemplazar saltos de línea con espacios\n",
        "    text = re.sub('\\n', ' ', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Aplicar segunda limpieza\n",
        "print(\"Aplicando segunda ronda de limpieza...\")\n",
        "\n",
        "textos_finales = []\n",
        "for texto in data_clean_round1.transcript:\n",
        "    texto_final = clean_text_round2(texto)\n",
        "    textos_finales.append(texto_final)\n",
        "\n",
        "data_clean = pd.DataFrame(textos_finales, columns=['transcript'], index=data_clean_round1.index)\n",
        "print(\"Segunda limpieza completada\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nqDq-p6Mc2X"
      },
      "source": [
        "### Verificación de la Limpieza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JG9MEu-Mc2X"
      },
      "outputs": [],
      "source": [
        "# Comparar texto original vs. texto limpio\n",
        "print(\"Comparación: Texto Original vs. Texto Limpio\")\n",
        "print(\"=\" * 60)\n",
        "print(\"ORIGINAL (2004, primeros 300 caracteres):\")\n",
        "print(data_df.transcript['2004'][:300])\n",
        "print()\n",
        "print(\"LIMPIO (2004, primeros 300 caracteres):\")\n",
        "print(data_clean.transcript['2004'][:300])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_R-VW2saMc2X"
      },
      "outputs": [],
      "source": [
        "# Guardar el corpus limpio para uso futuro\n",
        "data_clean.to_pickle(path + \"corpus_limpio.pkl\")\n",
        "print(\"Corpus limpio guardado exitosamente\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wtq_5X_3Mc2X"
      },
      "source": [
        "### Pregunta de Reflexión\n",
        "\n",
        "**Compara los textos original y limpio:**\n",
        "1. ¿Qué diferencias principales observas?\n",
        "2. ¿Crees que perdimos información importante en el proceso?\n",
        "3. ¿Por qué es importante esta normalización para el análisis posterior?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqDaoeqeMc2X"
      },
      "source": [
        "## 5. Vectorización: De Texto a Números\n",
        "\n",
        "### Teoría: El Modelo Bag of Words\n",
        "\n",
        "Las computadoras no entienden palabras, pero sí números. El modelo **Bag of Words** (BoW) convierte texto en vectores numéricos:\n",
        "\n",
        "1. **Vocabulario**: Lista de todas las palabras únicas del corpus\n",
        "2. **Vector por documento**: Para cada documento, cuenta cuántas veces aparece cada palabra\n",
        "3. **Matriz documento-término**: Filas = documentos, Columnas = palabras\n",
        "\n",
        "**Ejemplo simple:**\n",
        "- Documento 1: \"gato come pescado\"\n",
        "- Documento 2: \"perro come carne\"\n",
        "- Vocabulario: [\"gato\", \"come\", \"pescado\", \"perro\", \"carne\"]\n",
        "\n",
        "| Documento | gato | come | pescado | perro | carne |\n",
        "|-----------|------|------|---------|-------|-------|\n",
        "| Doc 1     |  1   |  1   |    1    |   0   |   0   |\n",
        "| Doc 2     |  0   |  1   |    0    |   1   |   1   |\n",
        "\n",
        "### Stop Words: Filtrando el Ruido\n",
        "\n",
        "Las **stop words** son palabras muy frecuentes que aportan poco significado semántico:\n",
        "- Artículos: \"el\", \"la\", \"los\", \"las\"\n",
        "- Preposiciones: \"de\", \"en\", \"con\", \"por\"\n",
        "- Pronombres: \"yo\", \"tú\", \"él\", \"ella\"\n",
        "- Verbos auxiliares: \"ser\", \"estar\", \"haber\"\n",
        "\n",
        "Eliminarlas mejora la calidad del análisis al enfocarse en palabras con contenido semántico."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZAPDPylMc2X"
      },
      "outputs": [],
      "source": [
        "# Descargar recursos de NLTK si no están disponibles\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "    print(\"Recursos de stopwords ya disponibles\")\n",
        "except LookupError:\n",
        "    print(\"Descargando stopwords de NLTK...\")\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "# Cargar stopwords en español\n",
        "stopwords_spanish = nltk.corpus.stopwords.words('spanish')\n",
        "\n",
        "print(f\"Número de stopwords en español: {len(stopwords_spanish)}\")\n",
        "print(\"\\nPrimeras 20 stopwords:\")\n",
        "print(stopwords_spanish[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egFlkjrzMc2X"
      },
      "source": [
        "### Creación de la Matriz Documento-Término"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nujh2nWzMc2X"
      },
      "outputs": [],
      "source": [
        "# Crear el vectorizador con stopwords\n",
        "print(\"Creando vectorizador Bag of Words...\")\n",
        "\n",
        "# CountVectorizer convierte texto a matriz de conteos\n",
        "vectorizador = CountVectorizer(\n",
        "    stop_words=stopwords_spanish,  # Filtrar stopwords en español\n",
        "    lowercase=True,                # Ya está en minúsculas, pero por seguridad\n",
        "    token_pattern=r'\\b[a-záéíóúñü]+\\b'  # Solo palabras con letras españolas\n",
        ")\n",
        "\n",
        "print(\"Configuración del vectorizador:\")\n",
        "print(f\"- Stop words: {len(stopwords_spanish)} palabras filtradas\")\n",
        "print(f\"- Patrón de tokens: Solo letras del alfabeto español\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQSoktS5Mc2X"
      },
      "outputs": [],
      "source": [
        "# Aplicar vectorización al corpus limpio\n",
        "print(\"Vectorizando corpus...\")\n",
        "print(\"Esto puede tomar unos momentos debido al tamaño del corpus\")\n",
        "\n",
        "# Fit_transform ajusta el vectorizador y aplica la transformación\n",
        "matriz_documentos_terminos = vectorizador.fit_transform(data_clean.transcript)\n",
        "\n",
        "print(f\"Matriz creada exitosamente:\")\n",
        "print(f\"- Dimensiones: {matriz_documentos_terminos.shape[0]} documentos x {matriz_documentos_terminos.shape[1]} palabras únicas\")\n",
        "print(f\"- Tipo: {type(matriz_documentos_terminos).__name__} (matriz esparsa)\")\n",
        "print(f\"- Elementos no-cero: {matriz_documentos_terminos.nnz:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UUc-qYPMc2X"
      },
      "source": [
        "### Comprensión de Matrices Esparsas\n",
        "\n",
        "Una **matriz esparsa** es una representación eficiente donde solo se almacenan los valores diferentes de cero. Esto es crucial en text mining porque:\n",
        "\n",
        "- La mayoría de documentos usan solo una fracción pequeña del vocabulario total\n",
        "- Una matriz \"densa\" sería enorme y llena de ceros\n",
        "- Las matrices esparsas ahorran memoria y aceleran los cálculos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4IyeyCGMc2X"
      },
      "outputs": [],
      "source": [
        "# Convertir a DataFrame para facilitar el análisis\n",
        "print(\"Convirtiendo matriz esparsa a DataFrame...\")\n",
        "\n",
        "# Obtener nombres de las características (palabras)\n",
        "nombres_palabras = vectorizador.get_feature_names_out()\n",
        "\n",
        "# Convertir a matriz densa (¡cuidado con la memoria!)\n",
        "data_bow = pd.DataFrame(\n",
        "    matriz_documentos_terminos.toarray(),\n",
        "    columns=nombres_palabras,\n",
        "    index=data_clean.index\n",
        ")\n",
        "\n",
        "print(f\"DataFrame creado:\")\n",
        "print(f\"- Dimensiones: {data_bow.shape}\")\n",
        "print(f\"- Memoria aproximada: {data_bow.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
        "\n",
        "# Mostrar una muestra pequeña\n",
        "print(\"\\nMuestra de la matriz (primeras 5 palabras):\")\n",
        "print(data_bow.iloc[:, :5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAnlXc7EMc2X"
      },
      "source": [
        "### Pregunta de Reflexión\n",
        "\n",
        "**Observa la matriz documento-término:**\n",
        "1. ¿Por qué hay tantos ceros en la matriz?\n",
        "2. ¿Qué representa cada número en la matriz?\n",
        "3. ¿Cómo crees que esta representación nos ayudará a analizar los textos?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK3xXMmeMc2X"
      },
      "source": [
        "## 6. Análisis Exploratorio de Frecuencias\n",
        "\n",
        "### Identificación de Palabras Más Frecuentes\n",
        "\n",
        "Ahora que tenemos nuestros textos vectorizados, podemos comenzar a extraer insights. Empezaremos identificando las palabras más frecuentes por año."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUElS_zvMc2X"
      },
      "outputs": [],
      "source": [
        "# Transponer la matriz para facilitar el análisis por año\n",
        "data_transpuesta = data_bow.transpose()\n",
        "\n",
        "print(\"Matriz transpuesta para análisis:\")\n",
        "print(f\"- Filas (palabras): {data_transpuesta.shape[0]}\")\n",
        "print(f\"- Columnas (años): {data_transpuesta.shape[1]}\")\n",
        "print()\n",
        "print(\"Primeras 5 palabras y sus frecuencias por año:\")\n",
        "print(data_transpuesta.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5J0XPYLMc2X"
      },
      "outputs": [],
      "source": [
        "# Función para obtener las palabras más frecuentes por año\n",
        "def obtener_palabras_frecuentes(data_transpuesta, num_palabras=30):\n",
        "    \"\"\"\n",
        "    Obtiene las palabras más frecuentes para cada año.\n",
        "\n",
        "    Parámetros:\n",
        "    data_transpuesta: DataFrame con palabras como filas y años como columnas\n",
        "    num_palabras: Número de palabras más frecuentes a extraer\n",
        "\n",
        "    Retorna:\n",
        "    dict: Diccionario con año como clave y lista de (palabra, frecuencia) como valor\n",
        "    \"\"\"\n",
        "    palabras_por_anio = {}\n",
        "\n",
        "    for anio in data_transpuesta.columns:\n",
        "        # Ordenar palabras por frecuencia descendente\n",
        "        top_palabras = data_transpuesta[anio].sort_values(ascending=False).head(num_palabras)\n",
        "\n",
        "        # Convertir a lista de tuplas (palabra, frecuencia)\n",
        "        palabras_por_anio[anio] = list(zip(top_palabras.index, top_palabras.values))\n",
        "\n",
        "    return palabras_por_anio\n",
        "\n",
        "# Obtener las 30 palabras más frecuentes por año\n",
        "top_palabras_anio = obtener_palabras_frecuentes(data_transpuesta, 30)\n",
        "\n",
        "print(\"Palabras más frecuentes obtenidas para todos los años\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOLKortIMc2Y"
      },
      "outputs": [],
      "source": [
        "# Mostrar las 15 palabras más frecuentes por año\n",
        "print(\"TOP 15 PALABRAS MÁS FRECUENTES POR AÑO\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for anio, palabras_frecuencias in top_palabras_anio.items():\n",
        "    print(f\"\\n{anio}:\")\n",
        "\n",
        "    # Extraer solo las palabras (sin las frecuencias) de las primeras 15\n",
        "    palabras = [palabra for palabra, frecuencia in palabras_frecuencias[:15]]\n",
        "\n",
        "    # Mostrar en formato legible\n",
        "    print(\", \".join(palabras))\n",
        "\n",
        "    # Mostrar también la palabra más frecuente con su conteo\n",
        "    palabra_top, frecuencia_top = palabras_frecuencias[0]\n",
        "    print(f\"  (Más frecuente: '{palabra_top}' con {frecuencia_top} apariciones)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0F6BgdG4Mc2Y"
      },
      "source": [
        "### Identificación de Stop Words Adicionales\n",
        "\n",
        "Al observar las palabras más frecuentes, notarás que algunas aparecen consistentemente año tras año. Estas son candidatas para ser **stop words adicionales** específicas de nuestro corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UzP3uZ7Mc2Y"
      },
      "outputs": [],
      "source": [
        "# Analizar qué palabras aparecen frecuentemente en múltiples años\n",
        "print(\"Analizando palabras que aparecen frecuentemente en múltiples años...\")\n",
        "\n",
        "# Crear lista con todas las palabras frecuentes\n",
        "todas_las_palabras_frecuentes = []\n",
        "\n",
        "for anio in anios:\n",
        "    palabras_del_anio = [palabra for (palabra, frecuencia) in top_palabras_anio[anio]]\n",
        "    todas_las_palabras_frecuentes.extend(palabras_del_anio)\n",
        "\n",
        "# Contar cuántas veces aparece cada palabra en los tops\n",
        "contador_palabras = Counter(todas_las_palabras_frecuentes)\n",
        "\n",
        "print(\"\\nPalabras que aparecen en el top de múltiples años:\")\n",
        "print(\"(palabra: número de años en los que está en el top)\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for palabra, count in contador_palabras.most_common(25):\n",
        "    print(f\"{palabra}: {count} años\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkTpnQqgMc2Y"
      },
      "outputs": [],
      "source": [
        "# Definir criterio para stop words adicionales\n",
        "# Si una palabra aparece en el top de más de la mitad de los años (>6), la consideramos stop word\n",
        "umbral_stopword = len(anios) // 2  # 6 años de los 12 totales\n",
        "\n",
        "stopwords_adicionales = [\n",
        "    palabra for palabra, count in contador_palabras.most_common()\n",
        "    if count > umbral_stopword\n",
        "]\n",
        "\n",
        "print(f\"Stop words adicionales identificadas (aparecen en >{umbral_stopword} años):\")\n",
        "print(stopwords_adicionales)\n",
        "print(f\"\\nTotal de stop words adicionales: {len(stopwords_adicionales)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQE4jjFdMc2c"
      },
      "outputs": [],
      "source": [
        "# Agregar stop words específicas del contexto (nombres propios, palabras muy generales)\n",
        "stopwords_contextuales = [\n",
        "    # Nombres propios que aparecen frecuentemente\n",
        "    'alex', 'lucas', 'andrés', 'andres', 'mirta', 'jesús', 'pablo', 'pepe',\n",
        "    # Números escritos\n",
        "    'uno', 'dos', 'tres', 'cuatro', 'cinco',\n",
        "    # Palabras muy generales\n",
        "    'cosa', 'cosas', 'tan', 'así', 'asi', 'luego', 'quizá', 'todas', 'sólo',\n",
        "    # Palabras de posición temporal/espacial muy generales\n",
        "    'primer', 'primera'\n",
        "]\n",
        "\n",
        "# Combinar todas las listas de stop words\n",
        "stopwords_completas = stopwords_spanish + stopwords_adicionales + stopwords_contextuales\n",
        "\n",
        "print(f\"Stop words totales: {len(stopwords_completas)}\")\n",
        "print(f\"- NLTK español: {len(stopwords_spanish)}\")\n",
        "print(f\"- Adicionales del corpus: {len(stopwords_adicionales)}\")\n",
        "print(f\"- Contextuales: {len(stopwords_contextuales)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaop02neMc2c"
      },
      "source": [
        "### Re-vectorización con Stop Words Mejoradas\n",
        "\n",
        "Ahora que tenemos una lista más completa de stop words, re-vectorizaremos nuestro corpus para obtener resultados más limpios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0uItuM-Mc2c"
      },
      "outputs": [],
      "source": [
        "# Crear nuevo vectorizador con stop words ampliadas\n",
        "print(\"Re-vectorizando con stop words mejoradas...\")\n",
        "\n",
        "vectorizador_mejorado = CountVectorizer(\n",
        "    stop_words=stopwords_completas,\n",
        "    lowercase=True,\n",
        "    token_pattern=r'\\b[a-záéíóúñü]+\\b',\n",
        "    min_df=2  # Ignorar palabras que aparecen en menos de 2 documentos\n",
        ")\n",
        "\n",
        "# Aplicar nueva vectorización\n",
        "matriz_mejorada = vectorizador_mejorado.fit_transform(data_clean.transcript)\n",
        "\n",
        "print(f\"Nueva matriz creada:\")\n",
        "print(f\"- Dimensiones: {matriz_mejorada.shape[0]} documentos x {matriz_mejorada.shape[1]} palabras\")\n",
        "print(f\"- Reducción de vocabulario: {data_bow.shape[1] - matriz_mejorada.shape[1]} palabras eliminadas\")\n",
        "print(f\"- Porcentaje de reducción: {(1 - matriz_mejorada.shape[1]/data_bow.shape[1])*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bvju5I6jMc2c"
      },
      "outputs": [],
      "source": [
        "# Convertir a DataFrame la matriz mejorada\n",
        "nombres_palabras_mejorado = vectorizador_mejorado.get_feature_names_out()\n",
        "\n",
        "data_bow_final = pd.DataFrame(\n",
        "    matriz_mejorada.toarray(),\n",
        "    columns=nombres_palabras_mejorado,\n",
        "    index=data_clean.index\n",
        ")\n",
        "\n",
        "print(\"DataFrame final creado exitosamente\")\n",
        "print(f\"Vocabulario final: {len(nombres_palabras_mejorado)} palabras únicas\")\n",
        "\n",
        "# Guardar resultados\n",
        "data_bow_final.to_pickle(path + \"bow_matriz_final.pkl\")\n",
        "pickle.dump(vectorizador_mejorado, open(path + \"vectorizador_final.pkl\", \"wb\"))\n",
        "print(\"Archivos guardados exitosamente\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBGZ726fMc2c"
      },
      "source": [
        "### Pregunta de Reflexión\n",
        "\n",
        "**Después de la re-vectorización:**\n",
        "1. ¿Por qué es importante eliminar palabras muy frecuentes?\n",
        "2. ¿Qué tipo de información perdemos al eliminar stop words?\n",
        "3. ¿Cómo crees que esta \"limpieza\" mejorará nuestros resultados?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0H6LSeQMc2c"
      },
      "source": [
        "## 7. Visualización: Nubes de Palabras\n",
        "\n",
        "### Teoría: ¿Qué son las Nubes de Palabras?\n",
        "\n",
        "Las **nubes de palabras** (word clouds) son visualizaciones donde:\n",
        "- El **tamaño** de cada palabra es proporcional a su frecuencia\n",
        "- Las **palabras más grandes** son las más frecuentes\n",
        "- La **posición** y **color** son principalmente estéticos\n",
        "\n",
        "Son útiles para obtener una **impresión visual rápida** de los temas principales en un texto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-cBXNQvMc2c"
      },
      "outputs": [],
      "source": [
        "# Configuración para las nubes de palabras\n",
        "configuracion_wordcloud = {\n",
        "    'stopwords': set(stopwords_completas),  # Usar nuestras stop words\n",
        "    'background_color': 'white',\n",
        "    'colormap': 'Dark2',                    # Esquema de colores profesional\n",
        "    'max_font_size': 150,                   # Tamaño máximo de fuente\n",
        "    'random_state': 42,                     # Para resultados reproducibles\n",
        "    'width': 800,\n",
        "    'height': 600,\n",
        "    'max_words': 30                        # Máximo 100 palabras por nube\n",
        "}\n",
        "\n",
        "# Crear el generador de nubes de palabras\n",
        "generador_wordcloud = WordCloud(**configuracion_wordcloud)\n",
        "\n",
        "print(\"Configuración de nubes de palabras establecida\")\n",
        "print(f\"Máximo de palabras por nube: {configuracion_wordcloud['max_words']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StlwTptMMc2c"
      },
      "outputs": [],
      "source": [
        "# Crear nubes de palabras para cada año\n",
        "print(\"Generando nubes de palabras por año...\")\n",
        "\n",
        "# Configurar el tamaño de la figura\n",
        "plt.figure(figsize=(20, 16))\n",
        "plt.suptitle('Evolución del Vocabulario de Hernán Casciari (2004-2015)',\n",
        "             fontsize=20, fontweight='bold', y=0.98)\n",
        "\n",
        "# Crear una nube para cada año\n",
        "for i, anio in enumerate(anios):\n",
        "    # Obtener el texto del año\n",
        "    texto_anio = data_clean.transcript[anio]\n",
        "\n",
        "    # Generar la nube de palabras\n",
        "    nube_palabras = generador_wordcloud.generate(texto_anio)\n",
        "\n",
        "    # Crear subplot\n",
        "    plt.subplot(4, 3, i + 1)\n",
        "    plt.imshow(nube_palabras, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(f'{anio}', fontsize=16, fontweight='bold', pad=10)\n",
        "\n",
        "# Ajustar espaciado\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(top=0.95)\n",
        "plt.show()\n",
        "\n",
        "print(\"Nubes de palabras generadas exitosamente\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPjNuVxoMc2c"
      },
      "source": [
        "### Análisis de las Nubes de Palabras\n",
        "\n",
        "**Observaciones clave que puedes notar:**\n",
        "\n",
        "1. **Evolución temática**: ¿Cómo cambian las palabras principales a lo largo de los años?\n",
        "2. **Temas recurrentes**: ¿Qué palabras aparecen consistentemente?\n",
        "3. **Eventos específicos**: ¿Hay años con vocabulario muy particular?\n",
        "\n",
        "**Contexto biográfico importante:**\n",
        "- La hija de Casciari nació en 2004\n",
        "- Observa cómo evoluciona la presencia de palabras familiares\n",
        "- Su revista \"Orsai\" y su pasión por el fútbol son temas recurrentes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziYL0wxBMc2c"
      },
      "source": [
        "### Pregunta de Reflexión\n",
        "\n",
        "**Observando las nubes de palabras:**\n",
        "1. ¿Qué patrones temporales observas en el vocabulario de Casciari?\n",
        "2. ¿Cómo crees que los eventos de su vida personal influyeron en su escritura?\n",
        "3. ¿Qué temas parecen ser constantes en su obra?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUWTNidvMc2c"
      },
      "source": [
        "## 8. Análisis Estadístico del Corpus\n",
        "\n",
        "### Métricas de Diversidad Léxica\n",
        "\n",
        "Vamos a calcular estadísticas que nos ayuden a entender mejor el corpus y la evolución del estilo de Casciari."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjwvlJ_bMc2d"
      },
      "outputs": [],
      "source": [
        "# Datos adicionales sobre la estructura del corpus\n",
        "# (Estos datos provienen del análisis original del blog)\n",
        "posts_por_anio = [50, 27, 18, 50, 42, 22, 50, 33, 31, 17, 33, 13]\n",
        "\n",
        "print(\"Información sobre la estructura del corpus:\")\n",
        "print(\"-\" * 40)\n",
        "for i, anio in enumerate(anios):\n",
        "    print(f\"{anio}: {posts_por_anio[i]} posts\")\n",
        "\n",
        "print(f\"\\nTotal de posts en el corpus: {sum(posts_por_anio)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9a5A9DJWMc2d"
      },
      "outputs": [],
      "source": [
        "# Calcular estadísticas detalladas por año\n",
        "def calcular_estadisticas_corpus(data_bow_final, posts_por_anio):\n",
        "    \"\"\"\n",
        "    Calcula estadísticas comprehensivas del corpus por año.\n",
        "\n",
        "    Retorna:\n",
        "    DataFrame con estadísticas por año\n",
        "    \"\"\"\n",
        "    estadisticas = []\n",
        "\n",
        "    for i, anio in enumerate(anios):\n",
        "        # Obtener datos del año\n",
        "        datos_anio = data_bow_final.loc[anio]\n",
        "\n",
        "        # Calcular métricas\n",
        "        palabras_unicas = (datos_anio > 0).sum()  # Número de palabras diferentes\n",
        "        palabras_totales = datos_anio.sum()       # Número total de palabras\n",
        "        posts_anio = posts_por_anio[i]\n",
        "        promedio_palabras_por_post = palabras_totales / posts_anio\n",
        "        diversidad_lexica = palabras_unicas / palabras_totales  # Ratio de diversidad\n",
        "\n",
        "        # Palabra más frecuente\n",
        "        palabra_top = datos_anio.idxmax()\n",
        "        frecuencia_top = datos_anio.max()\n",
        "\n",
        "        estadisticas.append({\n",
        "            'Año': anio,\n",
        "            'Posts': posts_anio,\n",
        "            'Palabras_Unicas': palabras_unicas,\n",
        "            'Palabras_Totales': palabras_totales,\n",
        "            'Promedio_Palabras_Post': round(promedio_palabras_por_post, 1),\n",
        "            'Diversidad_Lexica': round(diversidad_lexica, 4),\n",
        "            'Palabra_Mas_Frecuente': palabra_top,\n",
        "            'Frecuencia_Top': frecuencia_top\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(estadisticas)\n",
        "\n",
        "# Calcular estadísticas\n",
        "estadisticas_corpus = calcular_estadisticas_corpus(data_bow_final, posts_por_anio)\n",
        "\n",
        "print(\"ESTADÍSTICAS DEL CORPUS POR AÑO\")\n",
        "print(\"=\" * 50)\n",
        "print(estadisticas_corpus.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eASj2l6OMc2d"
      },
      "source": [
        "### Visualización de Estadísticas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtKWkX5-Mc2d"
      },
      "outputs": [],
      "source": [
        "# Crear visualización de las estadísticas principales\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "fig.suptitle('Análisis Estadístico del Corpus de Hernán Casciari', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Gráfico 1: Número de posts por año\n",
        "axes[0, 0].bar(estadisticas_corpus['Año'], estadisticas_corpus['Posts'], color='skyblue')\n",
        "axes[0, 0].set_title('Posts por Año')\n",
        "axes[0, 0].set_ylabel('Número de Posts')\n",
        "axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Gráfico 2: Palabras únicas por año\n",
        "axes[0, 1].bar(estadisticas_corpus['Año'], estadisticas_corpus['Palabras_Unicas'], color='lightgreen')\n",
        "axes[0, 1].set_title('Vocabulario Único por Año')\n",
        "axes[0, 1].set_ylabel('Palabras Únicas')\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Gráfico 3: Palabras totales por año\n",
        "axes[0, 2].bar(estadisticas_corpus['Año'], estadisticas_corpus['Palabras_Totales'], color='salmon')\n",
        "axes[0, 2].set_title('Volumen Total de Palabras por Año')\n",
        "axes[0, 2].set_ylabel('Palabras Totales')\n",
        "axes[0, 2].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Gráfico 4: Promedio de palabras por post\n",
        "axes[1, 0].plot(estadisticas_corpus['Año'], estadisticas_corpus['Promedio_Palabras_Post'],\n",
        "                marker='o', linewidth=2, markersize=8, color='purple')\n",
        "axes[1, 0].set_title('Promedio de Palabras por Post')\n",
        "axes[1, 0].set_ylabel('Palabras por Post')\n",
        "axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Gráfico 5: Diversidad léxica\n",
        "axes[1, 1].plot(estadisticas_corpus['Año'], estadisticas_corpus['Diversidad_Lexica'],\n",
        "                marker='s', linewidth=2, markersize=8, color='orange')\n",
        "axes[1, 1].set_title('Diversidad Léxica por Año')\n",
        "axes[1, 1].set_ylabel('Ratio Diversidad')\n",
        "axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Gráfico 6: Comparativo temporal\n",
        "ax2 = axes[1, 2].twinx()\n",
        "line1 = axes[1, 2].plot(estadisticas_corpus['Año'], estadisticas_corpus['Palabras_Unicas'],\n",
        "                        'b-o', label='Palabras Únicas')\n",
        "line2 = ax2.plot(estadisticas_corpus['Año'], estadisticas_corpus['Posts'],\n",
        "                 'r-s', label='Posts')\n",
        "axes[1, 2].set_title('Vocabulario vs. Productividad')\n",
        "axes[1, 2].set_ylabel('Palabras Únicas', color='b')\n",
        "ax2.set_ylabel('Posts', color='r')\n",
        "axes[1, 2].tick_params(axis='x', rotation=45)\n",
        "axes[1, 2].tick_params(axis='y', labelcolor='b')\n",
        "ax2.tick_params(axis='y', labelcolor='r')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3nilfTuMc2d"
      },
      "source": [
        "### Análisis de Evolución Temporal\n",
        "\n",
        "Vamos a analizar cómo evolucionan ciertas palabras clave a lo largo del tiempo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5p56NpbMc2d"
      },
      "outputs": [],
      "source": [
        "# Seleccionar palabras interesantes para el análisis temporal\n",
        "palabras_interes = ['padre', 'madre', 'hija', 'casa', 'fútbol', 'libro', 'revista', 'orsai']\n",
        "\n",
        "# Verificar qué palabras están en nuestro vocabulario\n",
        "palabras_disponibles = [palabra for palabra in palabras_interes\n",
        "                       if palabra in data_bow_final.columns]\n",
        "\n",
        "print(f\"Palabras disponibles para análisis temporal: {palabras_disponibles}\")\n",
        "\n",
        "# Crear DataFrame para análisis temporal\n",
        "if palabras_disponibles:\n",
        "    evolucion_temporal = pd.DataFrame(index=anios)\n",
        "\n",
        "    for palabra in palabras_disponibles:\n",
        "        evolucion_temporal[palabra] = data_bow_final[palabra].values\n",
        "\n",
        "    print(\"\\nEvolución temporal de palabras clave:\")\n",
        "    print(evolucion_temporal)\n",
        "else:\n",
        "    print(\"No se encontraron las palabras de interés en el vocabulario final.\")\n",
        "    print(\"Esto puede deberse a que fueron filtradas como stop words o son muy infrecuentes.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVEOWE1lMc2d"
      },
      "outputs": [],
      "source": [
        "# Si tenemos palabras disponibles, crear visualización temporal\n",
        "if palabras_disponibles:\n",
        "    plt.figure(figsize=(14, 8))\n",
        "\n",
        "    for palabra in palabras_disponibles:\n",
        "        plt.plot(evolucion_temporal.index, evolucion_temporal[palabra],\n",
        "                marker='o', linewidth=2, label=palabra)\n",
        "\n",
        "    plt.title('Evolución Temporal de Palabras Clave en los Cuentos de Casciari',\n",
        "              fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Año')\n",
        "    plt.ylabel('Frecuencia')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    # Buscar palabras alternativas en el vocabulario\n",
        "    print(\"\\nBuscando palabras familiares alternativas en el vocabulario...\")\n",
        "    vocabulario = list(data_bow_final.columns)\n",
        "\n",
        "    # Buscar palabras que contengan ciertos términos\n",
        "    palabras_familia = [p for p in vocabulario if any(term in p for term in ['padr', 'madr', 'hij', 'fami'])]\n",
        "    palabras_escritura = [p for p in vocabulario if any(term in p for term in ['libro', 'escrib', 'text'])]\n",
        "\n",
        "    print(f\"Palabras relacionadas con familia: {palabras_familia[:10]}\")\n",
        "    print(f\"Palabras relacionadas con escritura: {palabras_escritura[:10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojWal2w_Mc2d"
      },
      "source": [
        "### Pregunta de Reflexión\n",
        "\n",
        "**Analizando las estadísticas:**\n",
        "1. ¿Qué años muestran mayor diversidad léxica? ¿A qué crees que se debe?\n",
        "2. ¿Hay correlación entre el número de posts y la riqueza del vocabulario?\n",
        "3. ¿Qué patrones temporales observas en el uso de palabras específicas?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2klwCm5tMc2d"
      },
      "source": [
        "## 9. Conclusiones y Reflexiones Finales\n",
        "\n",
        "### Resumen del Proceso\n",
        "\n",
        "En este laboratorio hemos completado un pipeline completo de **text mining**:\n",
        "\n",
        "1. **Carga de datos**: Importamos 12 años de cuentos de Hernán Casciari\n",
        "2. **Preprocesamiento**: Limpiamos el texto eliminando ruido\n",
        "3. **Tokenización**: Dividimos el texto en palabras individuales\n",
        "4. **Filtrado**: Eliminamos stop words generales y específicas del corpus\n",
        "5. **Vectorización**: Convertimos texto a representación numérica (Bag of Words)\n",
        "6. **Análisis**: Calculamos frecuencias y estadísticas temporales\n",
        "7. **Visualización**: Creamos nubes de palabras y gráficos informativos\n",
        "\n",
        "### Hallazgos Principal\n",
        "\n",
        "**Evolución temática**: Los datos muestran cómo la escritura de Casciari refleja los cambios en su vida personal, especialmente relacionados con la paternidad y su carrera como escritor.\n",
        "\n",
        "**Diversidad léxica**: Observamos variaciones en la riqueza del vocabulario que pueden relacionarse con diferentes períodos creativos y contextos vitales.\n",
        "\n",
        "**Temas recurrentes**: Ciertos temas (familia, escritura, fútbol) aparecen consistentemente, definiendo la identidad literaria del autor.\n",
        "\n",
        "### Limitaciones del Análisis\n",
        "\n",
        "Es importante reconocer las limitaciones de nuestro approach:\n",
        "\n",
        "1. **Bag of Words**: Perdemos información sobre el orden de las palabras y el contexto\n",
        "2. **Stop words**: Eliminamos información que podría ser relevante para análisis estilísticos\n",
        "3. **Frequency-based**: Solo consideramos frecuencia, no significado semántico\n",
        "4. **Temporal aggregation**: Cada año se trata como un único documento\n",
        "\n",
        "### Extensiones Posibles\n",
        "\n",
        "Este análisis podría extenderse con:\n",
        "\n",
        "- **TF-IDF**: Para identificar palabras más distintivas por período\n",
        "- **N-gramas**: Para capturar frases y expresiones completas\n",
        "- **Análisis de sentimientos**: Para entender la evolución emocional\n",
        "- **Topic modeling**: Para identificar temas latentes automáticamente\n",
        "- **Named Entity Recognition**: Para tracking sistemático de personas y lugares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deAiykPuMc2d"
      },
      "source": [
        "### Pregunta Final de Reflexión\n",
        "\n",
        "**Reflexiona sobre todo el proceso:**\n",
        "1. ¿Qué insights más interesantes obtuviste sobre Hernán Casciari como escritor?\n",
        "2. ¿Cómo podrías aplicar estas técnicas a otros corpus de texto?\n",
        "3. ¿Qué aspectos del análisis te gustaría profundizar más?\n",
        "4. ¿Qué preguntas nuevas surgieron que no pudimos responder con este análisis?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yEcylMGMc2d"
      },
      "source": [
        "## Glosario de Términos\n",
        "\n",
        "**Bag of Words (BoW)**: Modelo de representación de texto donde cada documento se representa como un vector de frecuencias de palabras, ignorando el orden y la estructura gramatical.\n",
        "\n",
        "**Corpus**: Colección grande y estructurada de textos utilizados para análisis lingüístico o de procesamiento de lenguaje natural.\n",
        "\n",
        "**CountVectorizer**: Herramienta de scikit-learn que convierte una colección de documentos de texto en una matriz de conteos de tokens.\n",
        "\n",
        "**Diversidad Léxica**: Medida que indica la riqueza del vocabulario en un texto, calculada como la razón entre palabras únicas y palabras totales.\n",
        "\n",
        "**Expresiones Regulares (Regex)**: Secuencias de caracteres que definen patrones de búsqueda, utilizadas para encontrar y manipular texto de manera flexible.\n",
        "\n",
        "**Matriz Documento-Término**: Representación matemática donde las filas corresponden a documentos y las columnas a términos (palabras), con valores que indican la frecuencia de cada término en cada documento.\n",
        "\n",
        "**Matriz Esparsa**: Estructura de datos que almacena eficientemente matrices con muchos valores cero, guardando solo los elementos no-cero y sus posiciones.\n",
        "\n",
        "**NLTK (Natural Language Toolkit)**: Biblioteca de Python que proporciona herramientas para trabajar con datos de lenguaje humano, incluyendo tokenización, parsing y clasificación.\n",
        "\n",
        "**Preprocesamiento de Texto**: Conjunto de técnicas para limpiar y normalizar texto antes del análisis, incluyendo eliminación de puntuación, conversión a minúsculas, y remoción de caracteres especiales.\n",
        "\n",
        "**Serialización (Pickling)**: Proceso de convertir objetos Python en un formato que puede ser almacenado en disco y posteriormente recuperado (deserializado).\n",
        "\n",
        "**Stop Words**: Palabras muy frecuentes en un idioma que típicamente se filtran en análisis de texto por aportar poco contenido semántico (ej: \"el\", \"de\", \"y\").\n",
        "\n",
        "**Text Mining (Minería de Texto)**: Proceso de extraer información útil y patrones de grandes volúmenes de texto no estructurado mediante técnicas computacionales.\n",
        "\n",
        "**Token**: Unidad básica de texto después de la segmentación, típicamente una palabra o símbolo individual.\n",
        "\n",
        "**Tokenización**: Proceso de dividir texto en unidades más pequeñas (tokens), generalmente palabras, para su posterior análisis.\n",
        "\n",
        "**Vectorización**: Proceso de convertir texto en representaciones numéricas (vectores) que pueden ser procesadas por algoritmos de machine learning.\n",
        "\n",
        "**Vocabulario**: Conjunto de todas las palabras únicas presentes en un corpus de texto.\n",
        "\n",
        "**Word Cloud (Nube de Palabras)**: Visualización donde las palabras se muestran con tamaños proporcionales a su frecuencia, permitiendo identificar rápidamente los términos más importantes en un texto.\n",
        "\n",
        "---\n",
        "\n",
        "*Fin del Laboratorio de Text Mining*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}