{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb9a9f03"
      },
      "source": [
        "# Guía Práctica: Vectorización de Texto con Bag of Words y TF-IDF\n",
        "\n",
        "**Objetivo de Aprendizaje:** Al finalizar este notebook, serás capaz de:\n",
        "\n",
        "* Entender la intuición detrás de las técnicas de vectorización Bag of Words (BoW) y TF-IDF.\n",
        "* Implementar ambas técnicas utilizando la librería scikit-learn.\n",
        "* Analizar y comparar las representaciones vectoriales resultantes.\n",
        "* Comprender la importancia de los N-gramas para capturar términos compuestos.\n",
        "\n",
        "**Contexto:** En el procesamiento de lenguaje natural (PLN), los modelos de machine learning no entienden palabras, entienden números. La vectorización es el proceso fundamental de convertir texto en una representación numérica (vectores) que los algoritmos puedan procesar."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 0. PREPARACIÓN DEL ENTORNO ---\n",
        "# Se importan las librerías necesarias para el análisis.\n",
        "\n",
        "# Scikit-learn para la vectorización\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Pandas para la manipulación y visualización de datos en formato de tabla (DataFrame)\n",
        "import pandas as pd\n",
        "\n",
        "# Numpy para operaciones numéricas eficientes\n",
        "import numpy as np\n",
        "\n",
        "# Matplotlib y Seaborn para la creación de gráficos\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# --- DEFINICIÓN DEL CORPUS ---\n",
        "# Un corpus es simplemente una colección de documentos de texto.\n",
        "# Para este ejemplo, usaremos un corpus pequeño y controlado para entender los conceptos.\n",
        "documentos = [\n",
        "    \"El perro muerde al hombre\",\n",
        "    \"El hombre muerde al perro\",\n",
        "    \"El perro come carne\",\n",
        "    \"El hombre come comida\"\n",
        "]\n",
        "\n",
        "print(\"Nuestro corpus de ejemplo:\")\n",
        "for i, doc in enumerate(documentos):\n",
        "    print(f\"Documento {i+1}: {doc}\")"
      ],
      "metadata": {
        "id": "4hBS6K4mEmgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e66f672f"
      },
      "source": [
        "## Sección 1: Bag of Words (BoW) - La \"Bolsa de Palabras\"\n",
        "\n",
        "### 1.1. Explicación Teórica\n",
        "\n",
        "La técnica Bag of Words es una de las formas más simples e intuitivas de vectorizar texto. Imagina que tomas un documento, metes todas sus palabras en una bolsa, la sacudes y luego haces un inventario de cuántas veces aparece cada palabra.\n",
        "\n",
        "**Características principales:**\n",
        "\n",
        "*   **Ignora el orden:** No le importa la gramática ni el orden de las palabras, solo su frecuencia. De ahí el nombre \"bolsa\".\n",
        "*   **Crea un vocabulario:** Primero, inspecciona todo el corpus para crear un diccionario de todas las palabras únicas que existen.\n",
        "*   **Vectoriza por conteo:** Cada documento se representa como un vector donde cada posición corresponde a una palabra del vocabulario, y el valor en esa posición es el número de veces que la palabra aparece en el documento.\n",
        "\n",
        "**Analogía:** Si el vocabulario es `['perro', 'muerde', 'hombre']` y el documento es \"El perro muerde al hombre\", su vector BoW (simplificado) sería `[1, 1, 1]`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1.2. Implementación de Bag of Words ---\n",
        "\n",
        "print(\"\\n--- IMPLEMENTANDO BAG OF WORDS ---\")\n",
        "\n",
        "# Paso 1: Inicializar el CountVectorizer.\n",
        "# Esta clase de scikit-learn se encarga de todo el proceso de BoW.\n",
        "count_vect = CountVectorizer()\n",
        "\n",
        "# Paso 2: Aprender el vocabulario y transformar el corpus.\n",
        "# El método .fit_transform() realiza dos acciones:\n",
        "# 1. fit(): Aprende el vocabulario de nuestro corpus de 'documentos'.\n",
        "# 2. transform(): Convierte cada documento en un vector de conteo.\n",
        "bow_rep = count_vect.fit_transform(documentos)\n",
        "\n",
        "# Paso 3: Analizar los resultados.\n",
        "print(\"\\nNuestro vocabulario:\", count_vect.vocabulary_)\n",
        "# Nota: Los números indican el índice de la columna en la matriz final.\n",
        "\n",
        "# Mostramos la matriz completa de una forma más legible usando Pandas.\n",
        "vocab = count_vect.get_feature_names_out()\n",
        "df_bow = pd.DataFrame(bow_rep.toarray(), columns=vocab, index=[f\"Doc {i+1}\" for i in range(len(documentos))])\n",
        "print(\"\\nMatriz completa de Bag of Words:\")\n",
        "print(df_bow)\n",
        "\n",
        "# Nota en el código: La matriz resultante es \"esparsa\" (sparse),\n",
        "# lo que significa que la mayoría de sus valores son cero.\n",
        "# .toarray() la convierte en una matriz densa (numpy array) para poder visualizarla."
      ],
      "metadata": {
        "id": "J5JzWzuaFAvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e5636c1"
      },
      "source": [
        "## Sección 2: TF-IDF - Ponderando la Importancia\n",
        "\n",
        "### 2.1. Explicación Teórica\n",
        "\n",
        "Un problema con Bag of Words es que da el mismo peso a todas las palabras. Palabras muy comunes como \"el\", \"es\", \"de\" (llamadas stop words) pueden dominar los conteos sin aportar mucho significado sobre el tema del texto.\n",
        "\n",
        "TF-IDF (Term Frequency-Inverse Document Frequency) es una mejora que asigna un \"peso\" o \"score\" a cada palabra, reflejando su importancia en un documento dentro de un corpus.\n",
        "\n",
        "Se calcula con dos componentes:\n",
        "\n",
        "*   **Term Frequency (TF):** Mide la frecuencia de una palabra en un documento. Es similar a BoW. (Nº de veces que aparece el término / Nº total de términos en el documento).\n",
        "*   **Inverse Document Frequency (IDF):** Mide cuán \"rara\" o \"única\" es una palabra en todo el corpus. Las palabras que aparecen en muchos documentos (como \"el\") tienen un IDF bajo, mientras que las palabras más específicas tienen un IDF alto. `log(Nº total de documentos / Nº de documentos que contienen el término)`.\n",
        "\n",
        "El peso final es TF * IDF. Una palabra tendrá un peso alto si aparece mucho en un documento, pero en pocos documentos del resto del corpus."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2.2. Implementación de TF-IDF ---\n",
        "\n",
        "print(\"\\n--- IMPLEMENTANDO TF-IDF ---\")\n",
        "\n",
        "# Paso 1: Inicializar el TfidfVectorizer.\n",
        "# Funciona de manera muy similar al CountVectorizer.\n",
        "tfidf_vect = TfidfVectorizer()\n",
        "\n",
        "# Paso 2: Aprender el vocabulario y transformar el corpus.\n",
        "tfidf_rep = tfidf_vect.fit_transform(documentos)\n",
        "\n",
        "# Paso 3: Analizar los resultados.\n",
        "print(\"\\nVocabulario TF-IDF (es el mismo que en BoW):\", tfidf_vect.vocabulary_)\n",
        "\n",
        "# Mostramos la matriz completa con los pesos TF-IDF.\n",
        "tfidf_vocab = tfidf_vect.get_feature_names_out()\n",
        "df_tfidf = pd.DataFrame(tfidf_rep.toarray(), columns=tfidf_vocab, index=[f\"Doc {i+1}\" for i in range(len(documentos))])\n",
        "\n",
        "print(\"\\nMatriz completa de TF-IDF:\")\n",
        "print(df_tfidf.round(3)) # Redondeamos para una mejor visualización."
      ],
      "metadata": {
        "id": "FdAObvvwF4eD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cfd61ba"
      },
      "source": [
        "## Sección 3: Comparativa Visual y Análisis\n",
        "\n",
        "Ahora que tenemos ambas representaciones, comparemos directamente para entender la diferencia. Analizaremos el Documento 3: \"El perro come carne\"."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3.1. Visualización Comparativa ---\n",
        "print(\"\\n--- COMPARANDO BoW vs TF-IDF para 'El perro come carne' ---\")\n",
        "\n",
        "# Documento a analizar (índice 2)\n",
        "doc_idx = 2\n",
        "\n",
        "# DataFrames para la comparación\n",
        "df_comp_bow = pd.DataFrame({'palabra': vocab, 'valor': bow_rep[doc_idx].toarray()[0]})\n",
        "df_comp_tfidf = pd.DataFrame({'palabra': tfidf_vocab, 'valor': tfidf_rep[doc_idx].toarray()[0]})\n",
        "\n",
        "# Gráfico\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.barplot(x='palabra', y='valor', data=df_comp_bow.sort_values('valor', ascending=False))\n",
        "plt.title(f'Bag of Words - Doc {doc_idx+1}')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.ylabel(\"Conteo\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.barplot(x='palabra', y='valor', data=df_comp_tfidf.sort_values('valor', ascending=False))\n",
        "plt.title(f'TF-IDF - Doc {doc_idx+1}')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.ylabel(\"Peso TF-IDF\")\n",
        "\n",
        "plt.suptitle(f\"Análisis del Documento {doc_idx+1}: '{documentos[doc_idx]}'\", fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()\n",
        "\n",
        "# --- 3.2. Análisis de Resultados ---\n",
        "print(\"\\nObservaciones:\")\n",
        "print(\"- En BoW, todas las palabras presentes en el documento ('carne', 'come', 'el', 'perro') tienen un valor de 1.\")\n",
        "print(\"- En TF-IDF, las palabras 'carne' y 'come' tienen un peso mayor que 'perro' y 'el'.\")\n",
        "print(\"  Esto ocurre porque 'perro' y 'el' aparecen en más documentos, lo que las hace menos discriminativas (IDF más bajo).\")\n",
        "print(\"  TF-IDF identifica 'carne' y 'come' como las palabras más importantes para definir este documento en particular.\")"
      ],
      "metadata": {
        "id": "UUJz69QyGPVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f07da43"
      },
      "source": [
        "## Sección 4: Yendo más allá - N-gramas\n",
        "\n",
        "### 4.1. Explicación Teórica\n",
        "\n",
        "Hasta ahora, hemos tratado cada palabra de forma individual (esto se llama usar unigramas). Esto es una limitación, ya que perdemos el significado de términos compuestos como \"Buenos Aires\" o \"inteligencia artificial\".\n",
        "\n",
        "Los N-gramas son secuencias contiguas de N ítems (palabras) de una muestra de texto.\n",
        "\n",
        "*   **Bigramas (N=2):** Pares de palabras (\"Buenos Aires\").\n",
        "*   **Trigramas (N=3):** Tríos de palabras (\"Mar del Plata\").\n",
        "\n",
        "Al incluir N-gramas en nuestro vocabulario, podemos capturar estos términos compuestos como si fueran un solo token."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4.2. Implementación de N-gramas (Versión para principiantes) ---\n",
        "print(\"\\n--- IMPLEMENTANDO N-GRAMAS ---\")\n",
        "\n",
        "# Corpus con términos compuestos\n",
        "corpus_ngram = [\n",
        "    \"Buenos Aires es la capital de Argentina\",\n",
        "    \"Visité Mar del Plata y Buenos Aires el verano pasado\"\n",
        "]"
      ],
      "metadata": {
        "id": "mvGAKB0AICfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------\n",
        "# 1. Problema: Vectorizador estándar (solo unigramas)\n",
        "# ----------------------------------------------------------------\n",
        "print(\"\\n1. Analizando con palabras sueltas (Unigramas)\")\n",
        "\n",
        "vectorizer_unigram = CountVectorizer()\n",
        "X_unigram = vectorizer_unigram.fit_transform(corpus_ngram)\n",
        "\n",
        "# a) Obtenemos el vocabulario completo generado\n",
        "vocabulario_completo_unigramas = vectorizer_unigram.get_feature_names_out()\n",
        "\n",
        "# b) Creamos una lista vacía para guardar solo las palabras que nos interesan\n",
        "vocabulario_filtrado_unigramas = []\n",
        "\n",
        "# c) Definimos la lista de palabras que queremos buscar\n",
        "palabras_a_buscar = ['buenos', 'aires', 'mar', 'del', 'plata']\n",
        "\n",
        "# d) Recorremos CADA palabra en el vocabulario completo\n",
        "for palabra in vocabulario_completo_unigramas:\n",
        "    # e) Nos preguntamos: ¿Está esta 'palabra' en nuestra lista de 'palabras_a_buscar'?\n",
        "    if palabra in palabras_a_buscar:\n",
        "        # f) Si la respuesta es sí, la añadimos a nuestra lista filtrada\n",
        "        vocabulario_filtrado_unigramas.append(palabra)\n",
        "\n",
        "print(\"\\nVocabulario con Unigramas (extracto):\", vocabulario_filtrado_unigramas)\n",
        "print(\"-> 'Buenos' y 'Aires' son tokens separados. Se pierde el concepto de la ciudad.\")"
      ],
      "metadata": {
        "id": "_a6iHWAKG8Ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\n2. Analizando con Unigramas, Bigramas y Trigramas\")\n",
        "\n",
        "# ngram_range=(1, 3) incluirá tokens de 1, 2 y 3 palabras.\n",
        "vectorizer_trigram = CountVectorizer(ngram_range=(1, 3))\n",
        "X_trigram = vectorizer_trigram.fit_transform(corpus_ngram)\n",
        "vocab_trigram = vectorizer_trigram.get_feature_names_out()\n",
        "\n",
        "print(\"\\nVocabulario con Uni, Bi y Trigramas (extracto):\")\n",
        "terminos_interes = ['buenos', 'aires', 'buenos aires', 'mar', 'del', 'plata', 'mar del', 'del plata', 'mar del plata']\n",
        "\n",
        "# a) Creamos una nueva lista vacía\n",
        "vocabulario_filtrado_ngramas = []\n",
        "\n",
        "# b) Recorremos CADA término en el nuevo vocabulario (que ahora tiene n-gramas)\n",
        "for termino in vocab_trigram:\n",
        "    # c) Nos preguntamos: ¿Está este 'termino' en nuestra lista de 'terminos_interes'?\n",
        "    if termino in terminos_interes:\n",
        "        # d) Si la respuesta es sí, lo añadimos a nuestra lista filtrada\n",
        "        vocabulario_filtrado_ngramas.append(termino)\n",
        "\n",
        "print(vocabulario_filtrado_ngramas)\n",
        "print(\"-> Ahora 'buenos aires' y 'mar del plata' son tokens únicos en nuestro vocabulario.\")\n",
        "\n",
        "# Mostramos la matriz resultante para el segundo documento\n",
        "df_ngram = pd.DataFrame(X_trigram.toarray(), columns=vocab_trigram, index=[\"Doc 1\", \"Doc 2\"])\n",
        "print(\"\\nMatriz de conteo con N-gramas para el Doc 2 (extracto):\")\n",
        "print(df_ngram.loc[\"Doc 2\", terminos_interes])"
      ],
      "metadata": {
        "id": "AuJ_YsVkH-uW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71eb4a06"
      },
      "source": [
        "## Conclusiones Clave\n",
        "\n",
        "*   **Vectorizar es Traducir:** Hemos aprendido a \"traducir\" texto a un formato numérico que las máquinas pueden entender.\n",
        "*   **BoW es simple y rápido:** Ideal como punto de partida, cuenta la frecuencia de las palabras.\n",
        "*   **TF-IDF es más inteligente:** Pondera las palabras por su importancia real, dando más peso a los términos específicos y menos a los comunes.\n",
        "*   **El contexto importa (a veces):** Ambas técnicas ignoran el orden de las palabras, pero los N-gramas nos permiten recuperar parte de ese contexto al tratar términos compuestos como una sola unidad.\n",
        "*   **Limitación:** Las palabras que no están en el vocabulario aprendido durante el entrenamiento (`fit`) serán ignoradas al procesar texto nuevo (`transform`).\n",
        "\n",
        "## Glosario de Términos\n",
        "\n",
        "*   **Corpus:** Una colección de documentos de texto utilizados para entrenar un modelo o realizar un análisis.\n",
        "*   **Vectorización:** El proceso de convertir texto en vectores numéricos.\n",
        "*   **Token:** Una unidad de texto, generalmente una palabra o un signo de puntuación.\n",
        "*   **Vocabulario:** El conjunto de todos los tokens únicos presentes en un corpus.\n",
        "*   **Bag of Words (BoW):** Un modelo de representación de texto que describe un documento a través del conteo de sus palabras, ignorando el orden.\n",
        "*   **TF-IDF (Term Frequency-Inverse Document Frequency):** Una métrica numérica que refleja la importancia de una palabra en un documento dentro de una colección o corpus.\n",
        "*   **N-grama:** Una secuencia contigua de N palabras en un texto. Unigrama (N=1), Bigrama (N=2), Trigrama (N=3).\n",
        "*   **Matriz Esparsa (Sparse Matrix):** Una matriz en la que la mayoría de los elementos son cero. Es una forma eficiente de almacenar las representaciones vectoriales de texto."
      ]
    }
  ]
}